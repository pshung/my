
# Batch normalization.  

# Activation functio.   
Why? and comparison.  
https://www.aitude.com/comparison-of-sigmoid-tanh-and-relu-activation-functions/  

# representation learning / feature extraction.
Deep learning can internally build the representation of patterns in the data set.  
It partially replaces the need of feature engineering.  
subsequent layers build increasingly sophisticated representations of the raw data, until we get to a stage where we can make predictions.   
所以, 前期的feature engineering 做的好 可以幫助加快訓練速度 e.g., CNN's convolution layer 並且有效提取出有效特徵. (line, angle.)
直到後面的fully-connected layers 再根據這些high level features 去做預測.  


# feature engineering.  

# error surface
給一個loss function (MSE ...etc)  可以根據weight's value range 劃出loss function's error surface   

# 
